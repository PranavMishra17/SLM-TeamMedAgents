# Token and Inference Metrics Summary Guide

## Overview

When running experiments with the `--all` flag, the system now generates **two separate consolidated summary files** for comprehensive analysis:

1. **`all_configurations_summary_run{N}.json`** - Accuracy metrics across all 6 configurations
2. **`all_config_token_summary_run{N}.json`** - Token usage and inference metrics across all 6 configurations

Both files use run numbering to prevent overwriting on subsequent runs.

---

## File Structure: `all_config_token_summary_run1.json`

### Top-Level Metadata
```json
{
  "dataset": "medqa",
  "n_questions": 20,
  "model": "gemma3_4b",
  "n_agents": 3,
  "timestamp": "2025-10-30T18:30:00.123456"
}
```

### Individual Configuration Stats
For each of the 6 configurations, the file contains:

```json
{
  "rank": 1,
  "name": "Shared Mental Model only",
  "folder": "1_smm",
  "components": ["smm"],

  "token_usage": {
    "total_input_tokens": 16902,
    "total_output_tokens": 77902,
    "total_tokens": 94804,
    "questions_processed": 20,
    "avg_input_tokens_per_question": 845.1,
    "avg_output_tokens_per_question": 3895.1,
    "avg_total_tokens_per_question": 4740.2
  },

  "api_calls": {
    "total_calls": 317,
    "avg_calls_per_question": 15.85
  },

  "timing": {
    "total_time_seconds": 2695.78,
    "avg_time_per_question": 134.79,
    "avg_recruit_time": 27.80,
    "avg_round1_time": 32.01,
    "avg_round2_time": 24.01,
    "avg_round3_time": 24.01,
    "avg_aggregation_time": 0.0007
  },

  "metadata": {
    "total_questions": 20,
    "avg_agents_per_question": 3.56
  }
}
```

### Aggregate Statistics (Combined Across All 6 Configs)

```json
"aggregate_stats": {
  // Total sums across all 6 configurations
  "total_input_tokens": 101412,
  "total_output_tokens": 467412,
  "total_tokens": 568824,
  "total_api_calls": 1902,
  "total_time_seconds": 16174.68,
  "total_questions_processed": 120,

  // Averages per configuration
  "avg_input_tokens_per_config": 16902.0,
  "avg_output_tokens_per_config": 77902.0,
  "avg_total_tokens_per_config": 94804.0,
  "avg_api_calls_per_config": 317.0,
  "avg_time_per_config": 2695.78,

  // Averages per question (across ALL configs)
  "avg_input_tokens_per_question": 845.1,
  "avg_output_tokens_per_question": 3895.1,
  "avg_total_tokens_per_question": 4740.2,
  "avg_api_calls_per_question": 15.85,
  "avg_time_per_question": 134.79
}
```

---

## Key Metrics Explained

### Token Usage Metrics

| Metric | Description |
|--------|-------------|
| `total_input_tokens` | Total tokens sent to API (prompts, context) |
| `total_output_tokens` | Total tokens generated by model (responses) |
| `total_tokens` | Sum of input + output tokens |
| `avg_total_tokens_per_question` | Average tokens consumed per question |

### API Call Metrics

| Metric | Description |
|--------|-------------|
| `total_calls` | Total number of API requests made |
| `avg_calls_per_question` | Average API calls needed per question |

### Timing Metrics

| Metric | Description |
|--------|-------------|
| `total_time_seconds` | Total execution time in seconds |
| `avg_time_per_question` | Average seconds per question |
| `avg_recruit_time` | Average time for agent recruitment phase |
| `avg_round1_time` | Average time for Round 1 execution |
| `avg_round2_time` | Average time for Round 2 execution |
| `avg_round3_time` | Average time for Round 3 (debate) |
| `avg_aggregation_time` | Average time for decision aggregation |

---

## Use Cases

### 1. Cost Estimation
```python
import json

with open('all_config_token_summary_run1.json') as f:
    data = json.load(f)

# Gemini Flash pricing (example: $0.075 per 1M input, $0.30 per 1M output)
input_cost = data['aggregate_stats']['total_input_tokens'] * 0.075 / 1_000_000
output_cost = data['aggregate_stats']['total_output_tokens'] * 0.30 / 1_000_000
total_cost = input_cost + output_cost

print(f"Total cost for all 6 configs: ${total_cost:.4f}")
```

### 2. Configuration Comparison
```python
# Compare token efficiency across configurations
configs = data['configurations']
for config in configs:
    name = config['name']
    tokens = config['token_usage']['avg_total_tokens_per_question']
    time = config['timing']['avg_time_per_question']
    print(f"{name}: {tokens:.0f} tokens/q, {time:.1f}s/q")
```

### 3. Identify Most Efficient Configuration
```python
# Find config with best token efficiency
configs = sorted(data['configurations'],
                 key=lambda x: x['token_usage']['avg_total_tokens_per_question'])
best = configs[0]
print(f"Most efficient: {best['name']} - {best['token_usage']['avg_total_tokens_per_question']:.0f} tokens/q")
```

### 4. Scaling Projections
```python
agg = data['aggregate_stats']
questions_per_config = agg['total_questions_processed'] / 6

# Project cost for 1000 questions
scale_factor = 1000 / questions_per_config
projected_tokens = agg['total_tokens'] * scale_factor
projected_time = agg['total_time_seconds'] * scale_factor / 3600  # hours

print(f"For 1000 questions (all configs):")
print(f"  Tokens: {projected_tokens:,.0f}")
print(f"  Time: {projected_time:.1f} hours")
```

---

## File Locations

### Single Configuration Run
```
multi-agent-gemma/results/
├── medqa/
│   └── medqa_20q_run1/
│       └── summary_report.json  ← Individual run metrics
```

### All Configurations Run (--all flag)
```
multi-agent-gemma/results/
├── medqa_20q_all/
│   ├── 1_smm/
│   │   └── summary_report.json
│   ├── 2_leadership/
│   │   └── summary_report.json
│   ├── 3_team_orientation/
│   │   └── summary_report.json
│   ├── 4_trust/
│   │   └── summary_report.json
│   ├── 5_mutual_monitoring/
│   │   └── summary_report.json
│   ├── 6_all_active/
│   │   └── summary_report.json
│   ├── all_configurations_summary_run1.json        ← Accuracy summary
│   └── all_config_token_summary_run1.json         ← Token/inference summary
```

---

## Run Numbering

Both summary files use automatic run numbering:
- **First run**: `_run1.json`
- **Second run**: `_run2.json`
- **Third run**: `_run3.json`
- etc.

This prevents overwriting previous results, allowing you to:
- Compare multiple runs with different seeds
- Track experiments over time
- Analyze variance across runs

---

## Example Analysis Workflow

```bash
# Run experiment with seed 200
python run_simulation_adk.py --dataset medqa --n-questions 20 --all --seed 200 --key 2

# Check results
cd multi-agent-gemma/results/medqa_20q_all/

# View accuracy summary
cat all_configurations_summary_run1.json

# View token/inference metrics
cat all_config_token_summary_run1.json

# Run again with different seed
python run_simulation_adk.py --dataset medqa --n-questions 20 --all --seed 300 --key 3

# Now you have:
# - all_configurations_summary_run1.json (seed 200)
# - all_configurations_summary_run2.json (seed 300)
# - all_config_token_summary_run1.json (seed 200)
# - all_config_token_summary_run2.json (seed 300)
```

---

## Notes

1. **Files are kept separate**: Accuracy metrics and token metrics are in different files for clarity
2. **Individual summaries preserved**: Each config's `summary_report.json` contains full details
3. **Aggregate stats calculated**: Both per-config and per-question averages provided
4. **No data loss**: All timing breakdowns (recruit, R1, R2, R3, aggregation) included
5. **Scalable**: Works with any number of questions and configurations

---

## Quick Reference

| File | Purpose | Key Contents |
|------|---------|--------------|
| `all_configurations_summary_run{N}.json` | Accuracy comparison | Overall accuracy, correct counts, component lists |
| `all_config_token_summary_run{N}.json` | Cost & performance analysis | Token usage, API calls, timing, aggregate stats |
| `{config}/summary_report.json` | Detailed individual run | Full metrics, per-question breakdowns, convergence, disagreement |
