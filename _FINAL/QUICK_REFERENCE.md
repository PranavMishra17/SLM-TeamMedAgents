# Quick Reference Card - Multi-Agent Medical Reasoning Results

## üèÜ Rankings

### By Accuracy (25q subset, seed=42)
```
1. PATH-VQA    60% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë (Vision)
2. PMC-VQA     56% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë (Vision)
3. MedBullets  48% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë (Text)
4. DDXPlus     44% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë (Text)
4. MedMCQA     44% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë (Text)
4. MMLU-Pro    44% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë (Text)
7. PubMedQA    40% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë (Text)
8. MedQA       28% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë (Text)
```

### By Speed (seconds per question)
```
1. PATH-VQA     32.2s  ‚ö°‚ö°‚ö°‚ö°‚ö°
2. MedMCQA      36.0s  ‚ö°‚ö°‚ö°‚ö°‚ö°
3. PMC-VQA      42.8s  ‚ö°‚ö°‚ö°‚ö°
4. MMLU-Pro     48.2s  ‚ö°‚ö°‚ö°
5. MedQA        48.8s  ‚ö°‚ö°‚ö°
6. PubMedQA     64.3s  ‚ö°‚ö°
7. MedBullets   83.2s  ‚ö°
8. DDXPlus     107.8s  ‚ö°
```

### By Convergence (agent agreement)
```
1. PATH-VQA    100% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
2. PMC-VQA      92% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë
2. PubMedQA     92% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë
4. MedBullets   84% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë
5. DDXPlus      60% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë
5. MedMCQA      60% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë
7. MedQA        48% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë
8. MMLU-Pro     40% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë
```

## üìä Category Comparison

### Vision vs Text
```
                Vision    Text
Accuracy        58.0%    41.7%   +16.3pp üèÜ
Convergence     96.0%    64.0%   +32.0pp üèÜ
Agreement       98.5%    69.0%   +29.5pp üèÜ
Time/Question   37.5s    64.6s   -27.1s  üèÜ
Agents/Question 2.68     3.20    -0.52   üèÜ
Output Tokens   1,758    3,338   -1,580  üèÜ

Winner: Vision across all metrics ‚úÖ
```

## üí∞ Token Economics

### Average Tokens per Question
```
Input Tokens (Text):    282  ‚ñì‚ñì‚ñì‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë
Input Tokens (Vision):  2,257 ‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì (includes ~2,000 image tokens)
Output Tokens (Text):   3,338 ‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì
Output Tokens (Vision): 1,758 ‚ñì‚ñì‚ñì‚ñì‚ñì‚ñë‚ñë‚ñë‚ñë‚ñë
```

### Cost Efficiency
```
Best:  PATH-VQA   103.2 tokens/sec
Good:  PMC-VQA    109.9 tokens/sec
       MedMCQA     86.5 tokens/sec
Fair:  MedQA       89.2 tokens/sec
Poor:  DDXPlus     35.2 tokens/sec
```

## üéØ Performance Matrix

```
                 Accuracy  Convergence  Speed
PATH-VQA  üèÜ      60%       100%       32.2s
PMC-VQA   ‚≠ê      56%       92%        42.8s
MedBullets        48%       84%        83.2s
DDXPlus           44%       60%       107.8s
MedMCQA           44%       60%        36.0s
MMLU-Pro          44%       40%        48.2s
PubMedQA  ‚ö†Ô∏è      40%       92%        64.3s  (False consensus)
MedQA     ‚ö†Ô∏è      28%       48%        48.8s  (Most challenging)
```

## üîë Key Insights

### ‚úÖ What Works
- **Multimodal grounding** ‚Üí +16.3% accuracy, 100% convergence
- **Vision tasks** ‚Üí Faster despite image processing
- **Fewer agents on vision** ‚Üí 2.7 avg vs 3.2 for text

### ‚ö†Ô∏è What Doesn't
- **Text-only USMLE (MedQA)** ‚Üí Only 28% accuracy
- **High convergence ‚â† accuracy** ‚Üí PubMedQA: 92% agree, 40% correct
- **Differential diagnosis** ‚Üí Slowest (107.8s per question)

### üí° Surprising Findings
- Vision needs 47% fewer output tokens than text
- Perfect agent alignment only achieved with multimodal
- Agent variance is 6-10% for vision vs 18-65% for text

## üìà Overall Stats

```
Questions:      200 total (8 datasets √ó 25 questions)
Correct:        91 (45.5% accuracy)
Total Tokens:   768,835
API Calls:      2,257
Total Time:     3h 11m
Avg Time/Q:     57.4 seconds
```

## üöÄ Recommendations

### Immediate
1. ‚úÖ Deploy multimodal for all vision tasks
2. üîß Investigate MedQA low performance (28%)
3. ‚ö° Optimize DDXPlus speed (107s ‚Üí target 50s)

### Strategic
1. üìä Scale vision datasets to full evaluation
2. üî¨ Analyze PubMedQA false consensus pattern
3. üéØ Improve text-only methods (16% accuracy gap)

## üéì Dataset Profiles

### PATH-VQA üèÜ (Best Overall)
- **What**: Pathology microscopy images
- **Performance**: 60% accuracy, 100% convergence
- **Speed**: 32.2s per question
- **Best for**: Visual pathology diagnosis

### PMC-VQA ‚≠ê (Strong)
- **What**: Medical literature figures
- **Performance**: 56% accuracy, 92% convergence
- **Speed**: 42.8s per question
- **Best for**: Chart/diagram interpretation

### MedQA ‚ö†Ô∏è (Challenging)
- **What**: USMLE-style questions
- **Performance**: 28% accuracy, 48% convergence
- **Speed**: 48.8s per question
- **Best for**: Nothing yet - needs improvement

### PubMedQA ‚ö†Ô∏è (False Consensus)
- **What**: Yes/No/Maybe biomedical questions
- **Performance**: 40% accuracy, 92% convergence
- **Speed**: 64.3s per question
- **Warning**: High agreement on wrong answers

---

**Last Updated**: 2025-10-23
**Sample Size**: 25 questions per dataset (seed=42)
**Model**: Gemma-3-4b-it Multi-Agent System
