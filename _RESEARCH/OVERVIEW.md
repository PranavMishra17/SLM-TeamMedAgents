# Small Language Models (SLMs) and SLM Agents: Structured Research Overview

---

## General Surveys and Overviews

- **[A Comprehensive Survey of Small Language Models in the Era of Large Language Models](https://arxiv.org/abs/2411.03350)** – Research Paper (Nov 2024)  
  Systematic survey of SLMs covering architectures, distillation, quantization, applications, LLM collaboration, and trustworthiness.  

- **[Small Language Models Can Still Pack a Punch](https://arxiv.org/html/2501.05465v1)** – Research Paper (Jan 2025)  
  Survey of ~160 papers introducing "effective size" metric and modified scaling laws, with insights on distillation, synthetic data, and optimization.  

- **[Small Language Models Survey: Measurements and Insights](https://www.researchgate.net/publication/384295444_Small_Language_Models_Survey_Measurements_and_Insights)** – Research Paper (Feb 2025 update)  
  Empirical benchmarking of 59 SLMs on-device, analyzing architectures, runtime costs, quantization, and scaling laws.  

- **[Finding the Right SLM for Your Needs](https://www.refuel.ai/blog-posts/finding-the-right-slm-for-your-needs---a-guide-to-small-language-models)** – Blog Article (Refuel.ai)  
  Practical guide for selecting SLMs (1–3B) with benchmarking and deployment-focused decision frameworks.  

---

## Healthcare Applications

- **[Small Language Models Learn Enhanced Reasoning Skills from Medical Textbooks](https://arxiv.org/html/2404.00376v1)** – Research Paper (Apr 2024)  
  Introduces Meerkat-7B trained on medical textbooks and synthetic CoT data, achieving USMLE-passing performance.  

- **[Open-Source Small Language Models for Personal Medical Assistant Chatbots](https://www.sciencedirect.com/science/article/pii/S2666521224000644)** – Research Paper (2024)  
  Privacy-preserving local deployment of medical chatbots using open-source SLMs, validated via hypertension self-management.  

- **[The Power of Small LLMs in Healthcare: A RAG Framework Alternative](https://www.johnsnowlabs.com/the-power-of-small-llms-in-healthcare-a-rag-framework-alternative-to-large-language-models)** – Blog Article (John Snow Labs, 2024)  
  Demonstrates healthcare-specific SLMs in a RAG framework outperforming GPT-4o for clinical Q&A and research tasks.  

---

## Reasoning and Chain-of-Thought

- **[Language Models Perform Reasoning via Chain of Thought](https://research.google/blog/language-models-perform-reasoning-via-chain-of-thought)** – Blog Article (Google Research)  
  Introduces CoT prompting as an emergent property enabling multi-step reasoning in large models.  

- **[Symbolic Chain-of-Thought Distillation](https://arxiv.org/abs/2306.14050)** – Research Paper (ACL 2023)  
  Proposes CoT distillation for small models (125M–1.3B), enabling step-by-step reasoning via teacher-student training.  

---

## Agentic AI

- **[Small Language Models are the Future of Agentic AI](https://arxiv.org/abs/2506.02153)** – Research Paper (2025, NVIDIA/Georgia Tech)  
  Position paper advocating SLMs as efficient, economical, and better suited for agentic AI than LLMs.  

---

## Technical Methods: Distillation and Fine-Tuning

- **[Knowledge Distillation: Principles, Algorithms, Applications](https://neptune.ai/blog/knowledge-distillation)** – Blog Article (Neptune.ai)  
  Overview of distillation techniques, algorithms, and applications across NLP, vision, and speech.  

- **[Fine-Tuning Gemma 2B: A Practical Guide](https://medium.com/@heyamit10/fine-tuning-gemma-2b-a-practical-guide-e4c25de43b2d)** – Tutorial Article (Medium)  
  Practical guide for dataset prep, training pipeline, and optimization when fine-tuning Gemma 2B.  

- **[Fine-tune & Run Gemma 3](https://unsloth.ai/blog/gemma3)** – Technical Blog (Unsloth AI)  
  Optimized support for Gemma 3 with float16 fixes, quantization, and efficient fine-tuning workflows.  

---

## Model Architectures and Families

- **[Gemma Explained: An Overview of Gemma Model Family Architectures](https://developers.googleblog.com/en/gemma-explained-overview-gemma-model-family-architectures)** – Blog Article (Google Developers)  
  Technical overview of Gemma family architectures including text, code, recurrent, and multimodal variants.  

- **[NVIDIA’s Hybrid: Combining Attention and State Space Models](https://syncedreview.com/2024/12/14/self-evolving-prompts-redefining-ai-alignment-with-deepmind-chicago-us-eva-framework-14)** – Article (Synced, 2024)  
  Presents NVIDIA’s Hymba hybrid-head architecture combining attention and SSMs for efficient SLM performance.  

---
